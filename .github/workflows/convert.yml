---
name: Extract plugin URLs & build sgmodule

on:
  push:
    branches: [main]
  schedule:
    - cron: '5,30,55 * * * *'

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    services:
      docker:
        image: xream/script-hub:latest
        ports: ['9100:9100', '9101:9101']

    env:
      MIRROR_RAW: https://github.com/bunizao/Mirrored/raw/main/Chores/js
      MIRROR_KEY: github.com/bunizao/Mirrored

    steps:
      - uses: actions/checkout@v3

      - name: Setup git identity
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email \
            "github-actions[bot]@users.noreply.github.com"

      - name: Install workflow tooling
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y \
            curl \
            jq \
            netcat-openbsd \
            nodejs

      - name: Download plugin catalog for conversion
        env:
          PROXY_BASE: ${{ secrets.PROXY_BASE }}
          LIST_URL_PRIMARY: ${{ secrets.LIST_URL_PRIMARY }}
          LIST_URL_BACKUP: ${{ secrets.LIST_URL_BACKUP }}
          USER_AGENT: 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
          ACCEPT_HEADER: 'application/json, text/plain, */*'
          REFERER: 'https://pluginhub.kelee.one/'
          DEBUG_CURL: '0'
        run: |
          set -euo pipefail

          mkdir -p Cache

          : "${PROXY_BASE:?PROXY_BASE secret required}"
          : "${LIST_URL_PRIMARY:?LIST_URL_PRIMARY secret required}"

          if [[ "$PROXY_BASE" != *"?url="* ]]; then
            PROXY_BASE="${PROXY_BASE%/}?url="
          fi

          ENC_PRIMARY="$(python3 -c 'import os, urllib.parse; print(urllib.parse.quote(os.environ["LIST_URL_PRIMARY"], safe=""))')"
          ENC_BACKUP=""
          if [[ -n "${LIST_URL_BACKUP:-}" ]]; then
            ENC_BACKUP="$(python3 -c 'import os, urllib.parse; u=os.environ.get("LIST_URL_BACKUP", ""); print(urllib.parse.quote(u, safe=""))')"
          fi

          echo "::add-mask::${LIST_URL_PRIMARY}"
          [[ -n "${LIST_URL_BACKUP:-}" ]] && echo "::add-mask::${LIST_URL_BACKUP}"
          echo "::add-mask::${ENC_PRIMARY}"
          [[ -n "${ENC_BACKUP}" ]] && echo "::add-mask::${ENC_BACKUP}"
          echo "::add-mask::${PROXY_BASE}${ENC_PRIMARY}"
          [[ -n "${ENC_BACKUP}" ]] && echo "::add-mask::${PROXY_BASE}${ENC_BACKUP}"

          urls=( "${PROXY_BASE}${ENC_PRIMARY}" )
          [[ -n "${ENC_BACKUP}" ]] && urls+=( "${PROXY_BASE}${ENC_BACKUP}" )

          base_flags=(
            --fail --show-error --location --compressed --http1.1
            --retry 5 --retry-all-errors --connect-timeout 20 --max-time 120
            -A "$USER_AGENT" -H "Accept: $ACCEPT_HEADER" -e "$REFERER"
          )

          try_clean_json() {
            python3 - "$1" "$2" <<'PY'
import sys, json, re
src_path, out_path = sys.argv[1], sys.argv[2]
src = open(src_path, 'rb').read().decode('utf-8', 'ignore')

def write_clean(obj):
    with open(out_path, 'w', encoding='utf-8') as fh:
        json.dump(obj, fh, ensure_ascii=False, separators=(',', ':'))

try:
    write_clean(json.loads(src))
    sys.exit(0)
except Exception:
    pass

match = re.search(r"```(?:json)?\s*({.*?})\s*```", src, re.S | re.I)
if match:
    try:
        write_clean(json.loads(match.group(1)))
        sys.exit(0)
    except Exception:
        pass

stack = []
best = None
start = None
for idx, ch in enumerate(src):
    if ch == '{':
        if not stack:
            start = idx
        stack.append(ch)
    elif ch == '}' and stack:
        stack.pop()
        if not stack and start is not None:
            segment = src[start:idx + 1]
            if best is None or len(segment) > len(best):
                best = segment
            start = None

if best:
    try:
        write_clean(json.loads(best.strip()))
        sys.exit(0)
    except Exception:
        pass

sys.exit(2)
PY
          }

          fetch_ok=false
          rm -f plugin_data.json plugin_data.raw curl_debug.log headers.tmp || true

          for u in "${urls[@]}"; do
            if [[ "${DEBUG_CURL}" == "1" ]]; then
              curl "${base_flags[@]}" "$u" -D headers.tmp -o plugin_data.raw -v 2>>curl_debug.log || true
            else
              curl "${base_flags[@]}" "$u" -D headers.tmp -o plugin_data.raw || true
            fi

            if [[ -s plugin_data.raw ]]; then
              if command -v jq >/dev/null 2>&1 && jq . >/dev/null 2>&1 < plugin_data.raw; then
                mv plugin_data.raw plugin_data.json
                echo "âœ“ Catalog downloaded (valid JSON) via proxy"
                fetch_ok=true; break
              else
                echo "::notice ::Downloaded body not strict-JSON; trying smart extractor"
                if try_clean_json plugin_data.raw plugin_data.json; then
                  echo "âœ“ Extracted JSON from text via proxy"
                  fetch_ok=true; break
                else
                  echo "::warning ::Smart extract failed on this source"
                  rm -f plugin_data.json || true
                fi
              fi
            else
              echo "::warning ::Empty response from proxy source"
            fi

            rm -f plugin_data.raw headers.tmp || true
          done

          rm -f plugin_data.raw headers.tmp || true

          if [[ "${fetch_ok}" != "true" ]]; then
            if [[ -s Cache/plugin_data.json ]]; then
              echo "::warning ::Using cached Cache/plugin_data.json (fallback mode)"
              cp Cache/plugin_data.json plugin_data.json
            else
              [[ -f curl_debug.log ]] && { echo "::group::curl debug"; tail -n +1 curl_debug.log || true; echo "::endgroup::"; }
              echo "::error ::Failed to download plugin catalog via proxy and no cache available"
              exit 1
            fi
          fi

          if [[ "${fetch_ok}" == "true" ]]; then
            cp plugin_data.json Cache/plugin_data.json
            echo "Catalog refreshed for conversion"
          fi

          mv plugin_data.json Cache/plugin_data.json

      - name: Wait for Script-Hub container (:9101)
        run: |
          for i in {1..20}; do
            nc -z localhost 9101 && exit 0
            sleep 5
          done
          echo "::error ::container not ready"; exit 1

      # ---------- 1. Prepare LNPlugin source list ----------
      - name: Generate plugin URLs from cached catalog
        run: |
          set -euo pipefail

          if [ ! -f "Cache/plugin_data.json" ]; then
            echo "::error ::Missing Cache/plugin_data.json. Run mirror-lnplugin workflow first."
            exit 1
          fi

          python3 scripts/extract_lnplugin_urls.py \
            --input Cache/plugin_data.json \
            --output plugin_urls.txt

          if [ ! -s plugin_urls.txt ]; then
            echo "::error ::No LNPlugin URLs discovered in cached catalog"
            exit 1
          fi

          plugin_count=$(wc -l < plugin_urls.txt)
          echo "Prepared $plugin_count remote LNPlugin URLs for conversion"
          cat plugin_urls.txt

      # ---------- 2. Ask Script-Hub to render .sgmodule files ----------
      - name: Generate sgmodule files
        run: |
          mkdir -p Chores/sgmodule
          find Chores/sgmodule -maxdepth 1 -type f \
            -name "*.lpx.sgmodule" -delete
          category="ðŸš« AD Block"
          enc_cat=$(printf '%s' "$category" | jq -sRr @uri)
          script_hub_host="localhost"
          script_hub_port=9101

          trim_control_chars() {
            printf '%s' "$1" | tr -d '\r\n\t'
          }

          while read -r plugin_url; do
            plugin_url="$(trim_control_chars "$plugin_url")"
            [ -z "$plugin_url" ] && continue

            url_no_query="${plugin_url%%[\?#]*}"
            base_name=$(basename "$url_no_query")
            base_name="$(trim_control_chars "$base_name")"

            name="$base_name"
            if [[ "$name" == *.lpx ]]; then
              name="${name%.lpx}"
            fi
            if [[ "$name" == *.plugin ]]; then
              name="${name%.plugin}"
            fi
            name="$(trim_control_chars "$name")"
            [ -z "$name" ] && name="$base_name"

            enc=$(printf '%s' "$name" | jq -sRr @uri)
            base_url="http://${script_hub_host}:${script_hub_port}/file/_start_/${plugin_url}/_end_/${enc}.sgmodule"
            query="type=loon-plugin&target=surge-module&category=$enc_cat"
            url="${base_url}?${query}"
            echo "â†“  $url"

            if curl -fSLo "Chores/sgmodule/$name.sgmodule" \
              -A "Surge Mac/2985" \
              --connect-timeout 30 \
              --max-time 120 \
              --retry 3 \
              --retry-delay 5 \
              --retry-all-errors \
              "$url"; then
              echo "âœ“ Downloaded $name.sgmodule"
            else
              echo "::warning ::Failed to download $name.sgmodule"
              rm -f "Chores/sgmodule/$name.sgmodule"
            fi
          done < plugin_urls.txt

      # ---------- 3. Mirror external JS files and rewrite links ----------
      - name: Mirror external JS & rewrite links
        shell: bash
        run: |
          set -e
          mkdir -p Chores/js

          # Abort early if no sgmodule files were generated
          if [ ! -d "Chores/sgmodule" ] \
            || [ -z "$(ls -A Chores/sgmodule 2>/dev/null)" ]; then
            echo "No sgmodule files found to process"
            exit 0
          fi

          echo "Generated sgmodule files:"
          ls -la Chores/sgmodule/

          # Extract every JavaScript URL referenced by the sgmodules
          echo "Searching for JS URLs..."

          # Use a precise regex to capture URLs that follow script-path
          script_regex='script-path\s*=\s*https?://[^[:space:],"]+\.js[^[:space:],"]*'
          find Chores/sgmodule -name "*.sgmodule" \
              -exec grep -hoE "$script_regex" {} \; 2>/dev/null \
              | sed -E 's/^script-path\s*=\s*//' \
            | sort -u > external-js-raw.txt || touch external-js-raw.txt

          echo "Found JavaScript URLs:"
          cat external-js-raw.txt || echo "No JS URLs found"

          # Filter out URLs that already point at the mirror
          if [ -s external-js-raw.txt ]; then
            grep -v "$MIRROR_KEY" external-js-raw.txt \
              | grep -v "$MIRROR_RAW" > external-js.txt \
              || touch external-js.txt
          else
            touch external-js.txt
          fi

          echo "External JS URLs to mirror:"
          cat external-js.txt || echo "No external JS URLs to mirror"

          # Nothing to mirror? Clean up and exit early
          if [ ! -s external-js.txt ]; then
            echo "No external JS links found to mirror."
            rm -f external-js.txt external-js-raw.txt
            exit 0
          fi

          # Download and mirror the JS files
          echo "Processing external JS files:"
          failed_downloads=0
          while IFS= read -r url; do
            [ -z "$url" ] && continue

            # Extract a filename while stripping any query string
            filename=$(basename "${url%%[\?#]*}")
            file_path="Chores/js/$filename"

            # Ensure the filename ends with .js
            if [[ ! "$filename" =~ \.js$ ]]; then
              filename="${filename}.js"
              file_path="Chores/js/$filename"
            fi

            echo "Mirroring: $url â†’ Chores/js/$filename"

            if curl -Ls -A "Surge Mac/3272" \
              --connect-timeout 30 \
              --max-time 60 \
              "$url" \
              -o "$file_path"; then
              # Ensure the mirrored file has meaningful content
              if [ -s "$file_path" ]; then
                file_size=$(stat -c%s "$file_path")
                if [ "$file_size" -gt 10 ]; then
                  echo "âœ“ Successfully downloaded $filename"

                  # Rewrite every sgmodule reference with the mirrored URL
                  mirror_url="$MIRROR_RAW/$filename"
                  perl_expr="s,\\Q${url}\\E,${mirror_url},g"
                  find Chores/sgmodule -name "*.sgmodule" -type f \
                    -exec perl -pi -e "$perl_expr" {} \;

                  echo "âœ“ Updated links: $url â†’ $mirror_url"
                else
                  echo "âœ— Downloaded file is empty or too small: $filename"
                  rm -f "$file_path"
                  ((failed_downloads++))
                fi
              else
                echo "âœ— Downloaded file is empty or too small: $filename"
                rm -f "$file_path"
                ((failed_downloads++))
              fi
            else
              echo "âœ— Failed to download: $url"
              ((failed_downloads++))
            fi
          done < external-js.txt

          echo "Processing complete. Failed downloads: $failed_downloads"

          # Remove temporary files
          rm -f external-js.txt external-js-raw.txt

      - name: Commit & push (if changed)
        shell: bash
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -e
          git config user.name "github-actions[bot]"
          git config user.email \
            "github-actions[bot]@users.noreply.github.com"

          # Skip committing when nothing was generated
          if [ ! -d "Chores/sgmodule" ] && [ ! -d "Chores/js" ]; then
            echo "No files to commit."
            exit 0
          fi

          # Stage managed artifacts
          git add Chores/sgmodule Chores/js 2>/dev/null || true

          # Bail out if staging area is empty
          if git diff --cached --quiet; then
            echo "Nothing to commit."
            exit 0
          fi

          # Show the staged changes
          echo "Changes to be committed:"
          git diff --cached --name-status

          # Sync with the latest upstream history
          git fetch origin
          if git diff --quiet HEAD origin/main; then
            echo "Local branch is up to date with origin/main"
          else
            echo "Pulling latest changes from origin..."
            git pull --rebase origin main

            # Re-stage files after a successful rebase
            git add Chores/sgmodule Chores/js 2>/dev/null || true

            # Abort if nothing remains staged after rebase
            if git diff --cached --quiet; then
              echo "Nothing to commit after rebase."
              exit 0
            fi
          fi

          # Commit and push the new artifacts
          timestamp=$(date -u '+%Y-%m-%d %H:%M:%S UTC')
          commit_msg="Update sgmodule & mirror JS files - ${timestamp}"
          git commit -m "$commit_msg"

          echo "Pushing changes..."
          git push origin main

          echo "âœ“ Successfully committed and pushed changes"
