import os
import re
import requests
import json
from datetime import datetime
from urllib.parse import quote

# é…ç½®è¦ä¸‹è½½çš„ .sgmodule æ–‡ä»¶çš„ URL åˆ—è¡¨å’Œå¯¹åº”çš„æ¨¡å—æ ‡è¯†ï¼ˆç”¨äºæ³¨é‡Šï¼‰
sgmodule_info = [
    {
        'url': 'https://github.com/bunizao/Mirrored/raw/main/Chores/sgmodule/Zhihu_remove_ads.sgmodule',
        'header': 'Zhihu'
    },
    {
        'url': 'https://github.com/bunizao/Mirrored/raw/main/Chores/sgmodule/Amap_remove_ads.sgmodule',
        'header': 'Amap'
    },
    {
        'url': 'https://github.com/bunizao/Mirrored/raw/main/Chores/sgmodule/Weibo_remove_ads.sgmodule',
        'header': 'Weibo'
    },
    {
        'url': 'https://github.com/bunizao/Mirrored/raw/main/Chores/sgmodule/Tieba_remove_ads.sgmodule',
        'header': 'Tieba'
    },
    {
        'url': 'https://github.com/bunizao/Mirrored/raw/main/Chores/sgmodule/PinDuoDuo_remove_ads.sgmodule',
        'header': 'PinDuoDuo'
    },
    {
        'url': 'https://github.com/bunizao/Mirrored/raw/main/Chores/sgmodule/JD_remove_ads.sgmodule',
        'header': 'JD'
    },

# ç”Ÿæˆåˆå¹¶é“¾æ¥
base_url = 'https://script-hub.tutuis.me/file/_start_/'
end_url = '/_end_/Zhihu_remove_ads.sgmodule?type=surge-module&target=surge-module&del=true'
separator = 'ğŸ˜‚'

# æå– URL åˆ—è¡¨
module_urls = [module['url'] for module in sgmodule_info]

# å°† URL ä½¿ç”¨åˆ†éš”ç¬¦è¿æ¥
combined_urls = separator.join(module_urls)

# å¯¹åˆå¹¶çš„ URL è¿›è¡Œç¼–ç 
encoded_combined_urls = quote(combined_urls, safe=':/')

# æ„å»ºæœ€ç»ˆçš„åˆå¹¶é“¾æ¥
combined_link = f"{base_url}{encoded_combined_urls}{end_url}"

print(f"ç”Ÿæˆçš„åˆå¹¶é“¾æ¥ï¼š\n{combined_link}")

# å®šä¹‰è¦æå–çš„éƒ¨åˆ†
sections_to_extract = ['URL Rewrite', 'Script', 'Map Local']

# è·å–å½“å‰æ—¥æœŸï¼Œæ ¼å¼ä¸º MM/DD/YYYY
current_date = datetime.now().strftime('%m/%d/%Y')

# å­˜å‚¨åˆå¹¶çš„éƒ¨åˆ†å†…å®¹
merged_sections = {section: [] for section in sections_to_extract}

# ä¸‹è½½å¹¶è§£ææ¯ä¸ª .sgmodule æ–‡ä»¶
for module in sgmodule_info:
    url = module['url']
    header = module['header']
    print(f'æ­£åœ¨ä¸‹è½½ {url}')
    response = requests.get(url)
    if response.status_code == 200:
        content = response.text

        # æå–æŒ‡å®šçš„éƒ¨åˆ†
        pattern = re.compile(r'\[(.*?)\]\n((?:.|\n)*?)(?=\n\[|$)')
        matches = pattern.findall(content)

        for section_name, section_body in matches:
            section_name = section_name.strip()
            if section_name in sections_to_extract:
                lines = section_body.strip().split('\n')
                # åœ¨åˆå¹¶çš„éƒ¨åˆ†ä¸­æ·»åŠ æ¨¡å—æ¥æºçš„æ³¨é‡Š
                merged_sections[section_name].append(f'# -------------------------------------- {header} --------------------------------------')
                merged_sections[section_name].extend(lines)
                merged_sections[section_name].append('')  # æ·»åŠ ç©ºè¡Œï¼Œä¾¿äºé˜…è¯»
    else:
        print(f'ä¸‹è½½å¤±è´¥ {url}')

# ä»åˆå¹¶é“¾æ¥ä¸­è·å– [MITM] éƒ¨åˆ†
response = requests.get(combined_link)
if response.status_code == 200:
    content = response.text
    # æå– [MITM] éƒ¨åˆ†
    mitm_pattern = re.compile(r'\[MITM\]\n((?:.|\n)*?)(?=\n\[|$)')
    mitm_match = mitm_pattern.search(content)
    if mitm_match:
        mitm_content = mitm_match.group(1).strip()
        # å°† MITM ä¸»æœºååˆ—è¡¨è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œå¹¶å»é™¤ç©ºè¡Œ
        mitm_hosts = [line.strip() for line in mitm_content.split(',') if line.strip()]
        # å»é‡å¤„ç†
        mitm_hosts = list(dict.fromkeys(mitm_hosts))
        # æ·»åŠ åˆ° merged_sections
        merged_sections['MITM'] = mitm_hosts
    else:
        print('æœªæ‰¾åˆ° [MITM] éƒ¨åˆ†')
else:
    print('æ— æ³•è·å–åˆå¹¶é“¾æ¥çš„å†…å®¹')

# å»é‡å¤„ç†ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€è¦ï¼‰
for section in merged_sections:
    if section != 'MITM':
        # å»é™¤ç©ºå­—ç¬¦ä¸²
        merged_sections[section] = [line for line in merged_sections[section] if line.strip() != '']
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_lines = []
        for line in merged_sections[section]:
            if line not in seen:
                seen.add(line)
                unique_lines.append(line)
        merged_sections[section] = unique_lines

# å‡†å¤‡ä¼ é€’ç»™æ¨¡æ¿çš„æ•°æ®
template_data = {
    'currentDate': current_date,
    'mergedSections': merged_sections
}

# å°†æ•°æ®å†™å…¥ JSON æ–‡ä»¶
output_file = 'Chores/engineering/data/sgmodules_data.json'
os.makedirs(os.path.dirname(output_file), exist_ok=True)
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(template_data, f, ensure_ascii=False, indent=2)

print(f'æ•°æ®å·²ä¿å­˜åˆ° {output_file}')
